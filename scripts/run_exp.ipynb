{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T04:03:34.975707943Z",
     "start_time": "2023-07-15T04:03:34.909658832Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pmbrl.configs import get_config\n",
    "from scripts.train import main"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "运行稠密奖励实验如下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading experiment [device: cuda] ===\n",
      "\n",
      "{'action_noise': None,\n",
      " 'action_repeat': 3,\n",
      " 'batch_size': 50,\n",
      " 'coverage': False,\n",
      " 'ensemble_size': 25,\n",
      " 'env_name': 'SparseMountainCar',\n",
      " 'epsilon': 1e-08,\n",
      " 'expl_scale': 1.0,\n",
      " 'expl_strategy': 'information',\n",
      " 'grad_clip_norm': 1000,\n",
      " 'hidden_size': 200,\n",
      " 'learning_rate': 0.001,\n",
      " 'logdir': 'log',\n",
      " 'max_episode_len': 500,\n",
      " 'n_candidates': 500,\n",
      " 'n_episodes': 50,\n",
      " 'n_seed_episodes': 1,\n",
      " 'n_train_epochs': 100,\n",
      " 'optimisation_iters': 5,\n",
      " 'plan_horizon': 30,\n",
      " 'record_every': None,\n",
      " 'reset_interval': 1,\n",
      " 'reward_scale': 1.0,\n",
      " 'seed': 0,\n",
      " 'strategy': 'information',\n",
      " 'top_candidates': 50,\n",
      " 'use_exploration': True,\n",
      " 'use_mean': False,\n",
      " 'use_reward': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzh/miniconda3/envs/rl-inference/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected seeds: [1 episodes | 167 frames]\n",
      "\n",
      "=== Episode 1 ===\n",
      "Training on [167/501] data points\n",
      "> Train epoch 20 [ensemble -14.90 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -43.93 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -56.76 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -69.38 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -78.07 | reward 0.00]\n",
      "Ensemble loss -78.07 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [1] ===\n",
      "> Step 25 [reward 1.00]\n",
      "Rewards 1.00 / Steps 25.00\n",
      "Reward stats:\n",
      " {'max': '0.05', 'mean': '-0.06', 'min': '-0.20', 'std': '0.03'}\n",
      "Information gain stats:\n",
      " {'max': '97.84', 'mean': '51.79', 'min': '11.00', 'std': '17.39'}\n",
      "Episode time 8.23\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 2 ===\n",
      "Training on [192/576] data points\n",
      "> Train epoch 20 [ensemble 4.46 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -26.86 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -41.55 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -53.12 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -62.17 | reward 0.00]\n",
      "Ensemble loss -62.17 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [2] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 33.00\n",
      "Reward stats:\n",
      " {'max': '13.15', 'mean': '2.07', 'min': '-0.47', 'std': '2.70'}\n",
      "Information gain stats:\n",
      " {'max': '72.15', 'mean': '41.50', 'min': '10.07', 'std': '11.55'}\n",
      "Episode time 9.63\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 3 ===\n",
      "Training on [225/675] data points\n",
      "> Train epoch 20 [ensemble 3.41 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -23.31 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -34.70 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -43.13 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -51.01 | reward 0.00]\n",
      "Ensemble loss -51.01 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [3] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 28.00\n",
      "Reward stats:\n",
      " {'max': '17.38', 'mean': '1.93', 'min': '-0.08', 'std': '2.72'}\n",
      "Information gain stats:\n",
      " {'max': '99.16', 'mean': '40.27', 'min': '13.49', 'std': '12.52'}\n",
      "Episode time 8.51\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 4 ===\n",
      "Training on [253/759] data points\n",
      "> Train epoch 20 [ensemble -0.82 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -24.79 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -36.06 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -44.42 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -52.66 | reward 0.00]\n",
      "Ensemble loss -52.66 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [4] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 33.00\n",
      "Reward stats:\n",
      " {'max': '321.68', 'mean': '15.42', 'min': '-0.41', 'std': '40.65'}\n",
      "Information gain stats:\n",
      " {'max': '112.73', 'mean': '52.12', 'min': '9.54', 'std': '20.05'}\n",
      "Episode time 9.81\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 5 ===\n",
      "Training on [286/858] data points\n",
      "> Train epoch 20 [ensemble -0.85 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -24.62 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -35.47 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -43.16 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -49.83 | reward 0.00]\n",
      "Ensemble loss -49.83 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [5] ===\n",
      "Rewards 1.00 / Steps 24.00\n",
      "Reward stats:\n",
      " {'max': '3580.28', 'mean': '433.77', 'min': '0.01', 'std': '763.50'}\n",
      "Information gain stats:\n",
      " {'max': '162.70', 'mean': '67.63', 'min': '15.58', 'std': '36.58'}\n",
      "Episode time 7.61\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 6 ===\n",
      "Training on [310/930] data points\n",
      "> Train epoch 20 [ensemble -7.34 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -29.00 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -39.50 | reward 0.01]\n",
      "> Train epoch 80 [ensemble -47.05 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -54.55 | reward 0.00]\n",
      "Ensemble loss -54.55 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [6] ===\n",
      "Rewards 1.00 / Steps 24.00\n",
      "Reward stats:\n",
      " {'max': '11843.74', 'mean': '1456.64', 'min': '-0.38', 'std': '2649.59'}\n",
      "Information gain stats:\n",
      " {'max': '203.01', 'mean': '74.29', 'min': '10.28', 'std': '54.43'}\n",
      "Episode time 8.30\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 7 ===\n",
      "Training on [334/1002] data points\n",
      "> Train epoch 20 [ensemble -7.86 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -29.38 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -39.55 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -46.92 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -54.24 | reward 0.00]\n",
      "Ensemble loss -54.24 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [7] ===\n",
      "Rewards 1.00 / Steps 23.00\n",
      "Reward stats:\n",
      " {'max': '11712.00', 'mean': '1703.56', 'min': '-0.18', 'std': '2872.59'}\n",
      "Information gain stats:\n",
      " {'max': '190.65', 'mean': '72.52', 'min': '11.28', 'std': '50.19'}\n",
      "Episode time 8.17\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 8 ===\n",
      "Training on [357/1071] data points\n",
      "> Train epoch 20 [ensemble -11.88 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -32.24 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -42.19 | reward 0.01]\n",
      "> Train epoch 80 [ensemble -50.21 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -58.83 | reward 0.00]\n",
      "Ensemble loss -58.83 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [8] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 34.00\n",
      "Reward stats:\n",
      " {'max': '11522.62', 'mean': '1130.54', 'min': '0.03', 'std': '2407.05'}\n",
      "Information gain stats:\n",
      " {'max': '186.25', 'mean': '60.11', 'min': '9.76', 'std': '43.12'}\n",
      "Episode time 10.81\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 9 ===\n",
      "Training on [391/1173] data points\n",
      "> Train epoch 20 [ensemble -13.76 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -33.99 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -44.40 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -52.57 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -60.98 | reward 0.00]\n",
      "Ensemble loss -60.98 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [9] ===\n",
      "Rewards 1.00 / Steps 23.00\n",
      "Reward stats:\n",
      " {'max': '13685.91', 'mean': '1818.48', 'min': '-0.08', 'std': '3213.68'}\n",
      "Information gain stats:\n",
      " {'max': '190.41', 'mean': '67.34', 'min': '9.17', 'std': '51.01'}\n",
      "Episode time 7.50\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 10 ===\n",
      "Training on [414/1242] data points\n",
      "> Train epoch 20 [ensemble -18.89 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -37.44 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -47.51 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -56.70 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -65.65 | reward 0.00]\n",
      "Ensemble loss -65.65 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [10] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 33.00\n",
      "Reward stats:\n",
      " {'max': '13348.31', 'mean': '1327.92', 'min': '-0.15', 'std': '2817.32'}\n",
      "Information gain stats:\n",
      " {'max': '186.14', 'mean': '58.42', 'min': '8.36', 'std': '44.17'}\n",
      "Episode time 10.42\n",
      "Saved _metrics_\n",
      "saving buffer to buffer.npz, length with 447\n",
      "\n",
      "=== Episode 11 ===\n",
      "Training on [447/1341] data points\n",
      "> Train epoch 20 [ensemble -19.42 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -38.59 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -49.51 | reward 0.01]\n",
      "> Train epoch 80 [ensemble -59.46 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -68.53 | reward 0.00]\n",
      "Ensemble loss -68.53 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [11] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 26.00\n",
      "Reward stats:\n",
      " {'max': '11625.65', 'mean': '1511.71', 'min': '-0.18', 'std': '2744.55'}\n",
      "Information gain stats:\n",
      " {'max': '192.84', 'mean': '63.98', 'min': '7.60', 'std': '52.62'}\n",
      "Episode time 8.67\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 12 ===\n",
      "Training on [473/1419] data points\n",
      "> Train epoch 20 [ensemble -21.18 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -40.01 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -51.80 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -63.02 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -71.99 | reward 0.00]\n",
      "Ensemble loss -71.99 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [12] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "Rewards 1.00 / Steps 64.00\n",
      "Reward stats:\n",
      " {'max': '2129.80', 'mean': '64.16', 'min': '-0.17', 'std': '234.72'}\n",
      "Information gain stats:\n",
      " {'max': '129.51', 'mean': '30.52', 'min': '6.79', 'std': '26.41'}\n",
      "Episode time 19.14\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 13 ===\n",
      "Training on [537/1611] data points\n",
      "> Train epoch 20 [ensemble -22.79 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -42.21 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -56.62 | reward 0.01]\n",
      "> Train epoch 80 [ensemble -68.50 | reward 0.01]\n",
      "> Train epoch 100 [ensemble -76.78 | reward 0.00]\n",
      "Ensemble loss -76.78 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [13] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 29.00\n",
      "Reward stats:\n",
      " {'max': '6679.65', 'mean': '734.47', 'min': '-0.92', 'std': '1471.79'}\n",
      "Information gain stats:\n",
      " {'max': '160.93', 'mean': '46.52', 'min': '7.28', 'std': '41.00'}\n",
      "Episode time 9.85\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 14 ===\n",
      "Training on [566/1698] data points\n",
      "> Train epoch 20 [ensemble -24.79 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -44.72 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -60.26 | reward 0.01]\n",
      "> Train epoch 80 [ensemble -72.12 | reward 0.01]\n",
      "> Train epoch 100 [ensemble -79.94 | reward 0.00]\n",
      "Ensemble loss -79.94 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [14] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 49.00\n",
      "Reward stats:\n",
      " {'max': '7753.14', 'mean': '543.37', 'min': '-0.17', 'std': '1404.11'}\n",
      "Information gain stats:\n",
      " {'max': '170.66', 'mean': '45.03', 'min': '5.50', 'std': '35.44'}\n",
      "Episode time 15.66\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 15 ===\n",
      "Training on [615/1845] data points\n",
      "> Train epoch 20 [ensemble -27.84 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -48.76 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -64.73 | reward 0.01]\n",
      "> Train epoch 80 [ensemble -75.73 | reward 0.01]\n",
      "> Train epoch 100 [ensemble -83.00 | reward 0.00]\n",
      "Ensemble loss -83.00 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [15] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '1.19', 'mean': '0.09', 'min': '-0.10', 'std': '0.04'}\n",
      "Information gain stats:\n",
      " {'max': '28.52', 'mean': '16.88', 'min': '5.27', 'std': '2.63'}\n",
      "Episode time 49.10\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 16 ===\n",
      "Training on [782/2346] data points\n",
      "> Train epoch 20 [ensemble -35.66 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -61.05 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -76.42 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -85.32 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -91.17 | reward 0.00]\n",
      "Ensemble loss -91.17 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [16] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "Rewards 1.00 / Steps 51.00\n",
      "Reward stats:\n",
      " {'max': '24554.41', 'mean': '1467.07', 'min': '0.06', 'std': '4149.80'}\n",
      "Information gain stats:\n",
      " {'max': '220.84', 'mean': '44.99', 'min': '6.17', 'std': '50.69'}\n",
      "Episode time 17.03\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 17 ===\n",
      "Training on [833/2499] data points\n",
      "> Train epoch 20 [ensemble -37.15 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -63.60 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -78.17 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -86.55 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -92.05 | reward 0.00]\n",
      "Ensemble loss -92.05 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [17] ===\n",
      "Rewards 1.00 / Steps 24.00\n",
      "Reward stats:\n",
      " {'max': '6805.38', 'mean': '915.88', 'min': '-0.17', 'std': '1563.41'}\n",
      "Information gain stats:\n",
      " {'max': '183.76', 'mean': '60.62', 'min': '4.83', 'std': '49.28'}\n",
      "Episode time 9.76\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 18 ===\n",
      "Training on [857/2571] data points\n",
      "> Train epoch 20 [ensemble -39.30 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -66.46 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -80.64 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -88.68 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -93.95 | reward 0.00]\n",
      "Ensemble loss -93.95 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [18] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 37.00\n",
      "Reward stats:\n",
      " {'max': '15335.38', 'mean': '1262.47', 'min': '-0.27', 'std': '2981.58'}\n",
      "Information gain stats:\n",
      " {'max': '195.06', 'mean': '46.67', 'min': '4.20', 'std': '47.46'}\n",
      "Episode time 13.66\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 19 ===\n",
      "Training on [894/2682] data points\n",
      "> Train epoch 20 [ensemble -38.75 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -65.67 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -80.12 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -88.26 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -93.59 | reward 0.00]\n",
      "Ensemble loss -93.59 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [19] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 27.00\n",
      "Reward stats:\n",
      " {'max': '7749.59', 'mean': '810.18', 'min': '-0.28', 'std': '1621.02'}\n",
      "Information gain stats:\n",
      " {'max': '195.56', 'mean': '55.91', 'min': '7.44', 'std': '50.89'}\n",
      "Episode time 10.86\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 20 ===\n",
      "Training on [921/2763] data points\n",
      "> Train epoch 20 [ensemble -41.05 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -68.28 | reward 0.01]\n",
      "> Train epoch 60 [ensemble -81.89 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -89.65 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -94.75 | reward 0.00]\n",
      "Ensemble loss -94.75 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [20] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 46.00\n",
      "Reward stats:\n",
      " {'max': '8408.12', 'mean': '521.74', 'min': '-0.07', 'std': '1398.31'}\n",
      "Information gain stats:\n",
      " {'max': '202.47', 'mean': '46.88', 'min': '6.24', 'std': '45.30'}\n",
      "Episode time 16.20\n",
      "Saved _metrics_\n",
      "saving buffer to buffer.npz, length with 967\n",
      "\n",
      "=== Episode 21 ===\n",
      "Training on [967/2901] data points\n",
      "> Train epoch 20 [ensemble -43.95 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -71.00 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -84.12 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -91.57 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -96.46 | reward 0.00]\n",
      "Ensemble loss -96.46 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [21] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "Rewards 1.00 / Steps 53.00\n",
      "Reward stats:\n",
      " {'max': '6747.33', 'mean': '350.20', 'min': '-0.34', 'std': '1031.68'}\n",
      "Information gain stats:\n",
      " {'max': '216.65', 'mean': '43.63', 'min': '5.89', 'std': '47.04'}\n",
      "Episode time 18.27\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 22 ===\n",
      "Training on [1020/3060] data points\n",
      "> Train epoch 20 [ensemble -46.49 | reward 0.01]\n",
      "> Train epoch 40 [ensemble -73.23 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -85.92 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -93.10 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -97.79 | reward 0.00]\n",
      "Ensemble loss -97.79 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [22] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '2.48', 'mean': '-0.13', 'min': '-0.24', 'std': '0.02'}\n",
      "Information gain stats:\n",
      " {'max': '30.05', 'mean': '16.53', 'min': '5.18', 'std': '2.62'}\n",
      "Episode time 50.19\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 23 ===\n",
      "Training on [1187/3561] data points\n",
      "> Train epoch 20 [ensemble -55.09 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -80.41 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -91.32 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -97.38 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -101.33 | reward 0.00]\n",
      "Ensemble loss -101.33 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [23] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 47.00\n",
      "Reward stats:\n",
      " {'max': '67634.34', 'mean': '2252.43', 'min': '0.14', 'std': '7655.07'}\n",
      "Information gain stats:\n",
      " {'max': '257.60', 'mean': '46.27', 'min': '5.70', 'std': '59.56'}\n",
      "Episode time 17.67\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 24 ===\n",
      "Training on [1234/3702] data points\n",
      "> Train epoch 20 [ensemble -56.78 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -81.70 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -92.28 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -98.16 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -102.02 | reward 0.00]\n",
      "Ensemble loss -102.02 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [24] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '7.68', 'mean': '0.03', 'min': '-0.03', 'std': '0.02'}\n",
      "Information gain stats:\n",
      " {'max': '28.21', 'mean': '16.51', 'min': '5.08', 'std': '2.62'}\n",
      "Episode time 51.06\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 25 ===\n",
      "Training on [1401/4203] data points\n",
      "> Train epoch 20 [ensemble -64.84 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -87.09 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -96.19 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -101.29 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -104.63 | reward 0.00]\n",
      "Ensemble loss -104.63 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [25] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '1.38', 'mean': '0.02', 'min': '0.01', 'std': '0.00'}\n",
      "Information gain stats:\n",
      " {'max': '29.47', 'mean': '16.23', 'min': '3.68', 'std': '2.58'}\n",
      "Episode time 51.78\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 26 ===\n",
      "Training on [1568/4704] data points\n",
      "> Train epoch 20 [ensemble -70.74 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -90.46 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -98.50 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -103.05 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -106.08 | reward 0.00]\n",
      "Ensemble loss -106.08 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [26] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 46.00\n",
      "Reward stats:\n",
      " {'max': '1485305.00', 'mean': '43128.41', 'min': '0.01', 'std': '162162.30'}\n",
      "Information gain stats:\n",
      " {'max': '346.94', 'mean': '60.16', 'min': '6.46', 'std': '83.91'}\n",
      "Episode time 18.77\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 27 ===\n",
      "Training on [1614/4842] data points\n",
      "> Train epoch 20 [ensemble -71.08 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -90.85 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -98.80 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -103.32 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -106.24 | reward 0.00]\n",
      "Ensemble loss -106.24 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [27] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 33.00\n",
      "Reward stats:\n",
      " {'max': '568796.88', 'mean': '18805.86', 'min': '-3.08', 'std': '58673.59'}\n",
      "Information gain stats:\n",
      " {'max': '355.39', 'mean': '86.48', 'min': '8.82', 'std': '94.27'}\n",
      "Episode time 15.25\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 28 ===\n",
      "Training on [1647/4941] data points\n",
      "> Train epoch 20 [ensemble -71.07 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -90.69 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -98.67 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -103.23 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -106.22 | reward 0.00]\n",
      "Ensemble loss -106.22 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [28] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "Rewards 1.00 / Steps 110.00\n",
      "Reward stats:\n",
      " {'max': '476463.88', 'mean': '4337.19', 'min': '-2.48', 'std': '27356.84'}\n",
      "Information gain stats:\n",
      " {'max': '333.05', 'mean': '34.64', 'min': '5.42', 'std': '54.57'}\n",
      "Episode time 36.62\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 29 ===\n",
      "Training on [1757/5271] data points\n",
      "> Train epoch 20 [ensemble -74.76 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -92.80 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -100.19 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -104.41 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -107.19 | reward 0.00]\n",
      "Ensemble loss -107.19 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [29] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 33.00\n",
      "Reward stats:\n",
      " {'max': '3182019.50', 'mean': '95669.30', 'min': '-0.05', 'std': '313203.78'}\n",
      "Information gain stats:\n",
      " {'max': '384.73', 'mean': '90.16', 'min': '7.05', 'std': '103.12'}\n",
      "Episode time 16.09\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 30 ===\n",
      "Training on [1790/5370] data points\n",
      "> Train epoch 20 [ensemble -74.09 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -92.34 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -99.81 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -104.05 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -106.86 | reward 0.00]\n",
      "Ensemble loss -106.86 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [30] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '1.23', 'mean': '0.10', 'min': '0.02', 'std': '0.02'}\n",
      "Information gain stats:\n",
      " {'max': '28.25', 'mean': '16.33', 'min': '5.53', 'std': '2.60'}\n",
      "Episode time 53.40\n",
      "Saved _metrics_\n",
      "saving buffer to buffer.npz, length with 1957\n",
      "\n",
      "=== Episode 31 ===\n",
      "Training on [1957/5871] data points\n",
      "> Train epoch 20 [ensemble -78.75 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -95.12 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -101.83 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -105.69 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -108.21 | reward 0.00]\n",
      "Ensemble loss -108.21 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [31] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '0.02', 'mean': '-0.03', 'min': '-0.07', 'std': '0.01'}\n",
      "Information gain stats:\n",
      " {'max': '28.41', 'mean': '16.24', 'min': '5.23', 'std': '2.59'}\n",
      "Episode time 53.47\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 32 ===\n",
      "Training on [2124/6372] data points\n",
      "> Train epoch 20 [ensemble -81.68 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -96.92 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -103.16 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -106.74 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -109.12 | reward 0.00]\n",
      "Ensemble loss -109.12 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [32] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '-0.05', 'mean': '-0.12', 'min': '-34.91', 'std': '0.21'}\n",
      "Information gain stats:\n",
      " {'max': '39.26', 'mean': '16.31', 'min': '4.73', 'std': '2.66'}\n",
      "Episode time 52.22\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 33 ===\n",
      "Training on [2291/6873] data points\n",
      "> Train epoch 20 [ensemble -84.31 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -98.51 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -104.26 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -107.56 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -109.74 | reward 0.00]\n",
      "Ensemble loss -109.74 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [33] ===\n",
      "Rewards 1.00 / Steps 24.00\n",
      "Reward stats:\n",
      " {'max': '190074.38', 'mean': '7973.86', 'min': '-0.03', 'std': '23313.68'}\n",
      "Information gain stats:\n",
      " {'max': '383.10', 'mean': '94.03', 'min': '8.00', 'std': '104.90'}\n",
      "Episode time 14.55\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 34 ===\n",
      "Training on [2315/6945] data points\n",
      "> Train epoch 20 [ensemble -84.11 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -98.39 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -104.20 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -107.52 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -109.67 | reward 0.00]\n",
      "Ensemble loss -109.67 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [34] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 26.00\n",
      "Reward stats:\n",
      " {'max': '5829923.00', 'mean': '150990.12', 'min': '-0.30', 'std': '504643.06'}\n",
      "Information gain stats:\n",
      " {'max': '401.80', 'mean': '93.07', 'min': '8.19', 'std': '104.79'}\n",
      "Episode time 15.42\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 35 ===\n",
      "Training on [2341/7023] data points\n",
      "> Train epoch 20 [ensemble -84.89 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -98.63 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -104.28 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -107.60 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -109.75 | reward 0.00]\n",
      "Ensemble loss -109.75 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [35] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '-0.03', 'mean': '-0.17', 'min': '-0.41', 'std': '0.03'}\n",
      "Information gain stats:\n",
      " {'max': '28.57', 'mean': '15.95', 'min': '5.01', 'std': '2.57'}\n",
      "Episode time 53.44\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 36 ===\n",
      "Training on [2508/7524] data points\n",
      "> Train epoch 20 [ensemble -87.00 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.12 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.40 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.42 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.44 | reward 0.00]\n",
      "Ensemble loss -110.44 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [36] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 29.00\n",
      "Reward stats:\n",
      " {'max': '0.35', 'mean': '-862.30', 'min': '-348044.75', 'std': '5722.57'}\n",
      "Information gain stats:\n",
      " {'max': '250.88', 'mean': '49.01', 'min': '7.17', 'std': '31.57'}\n",
      "Episode time 17.24\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 37 ===\n",
      "Training on [2537/7611] data points\n",
      "> Train epoch 20 [ensemble -86.86 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -99.91 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.24 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.35 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.42 | reward 0.00]\n",
      "Ensemble loss -110.42 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [37] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "Rewards 1.00 / Steps 52.00\n",
      "Reward stats:\n",
      " {'max': '4.67', 'mean': '-1098.15', 'min': '-380728.06', 'std': '8550.73'}\n",
      "Information gain stats:\n",
      " {'max': '261.12', 'mean': '38.20', 'min': '6.79', 'std': '30.73'}\n",
      "Episode time 23.31\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 38 ===\n",
      "Training on [2589/7767] data points\n",
      "> Train epoch 20 [ensemble -87.15 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.02 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.28 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.35 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.37 | reward 0.00]\n",
      "Ensemble loss -110.37 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [38] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 26.00\n",
      "Reward stats:\n",
      " {'max': '235461.91', 'mean': '5397.97', 'min': '0.03', 'std': '20148.55'}\n",
      "Information gain stats:\n",
      " {'max': '342.06', 'mean': '64.64', 'min': '7.42', 'std': '80.80'}\n",
      "Episode time 16.60\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 39 ===\n",
      "Training on [2615/7845] data points\n",
      "> Train epoch 20 [ensemble -86.82 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -99.92 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.25 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.37 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.39 | reward 0.00]\n",
      "Ensemble loss -110.39 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [39] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 40.00\n",
      "Reward stats:\n",
      " {'max': '27.71', 'mean': '-327.99', 'min': '-146827.45', 'std': '2473.69'}\n",
      "Information gain stats:\n",
      " {'max': '268.50', 'mean': '50.85', 'min': '6.90', 'std': '33.23'}\n",
      "Episode time 20.46\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 40 ===\n",
      "Training on [2655/7965] data points\n",
      "> Train epoch 20 [ensemble -87.26 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.22 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.54 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.62 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.57 | reward 0.00]\n",
      "Ensemble loss -110.57 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [40] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 37.00\n",
      "Reward stats:\n",
      " {'max': '847036224.00',\n",
      " 'mean': '11054106.00',\n",
      " 'min': '-0.07',\n",
      " 'std': '49913076.00'}\n",
      "Information gain stats:\n",
      " {'max': '459.96', 'mean': '88.06', 'min': '6.50', 'std': '119.30'}\n",
      "Episode time 19.71\n",
      "Saved _metrics_\n",
      "saving buffer to buffer.npz, length with 2692\n",
      "\n",
      "=== Episode 41 ===\n",
      "Training on [2692/8076] data points\n",
      "> Train epoch 20 [ensemble -87.02 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.06 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.39 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.49 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.47 | reward 0.00]\n",
      "Ensemble loss -110.47 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [41] ===\n",
      "> Step 25 [reward 1.00]\n",
      "Rewards 1.00 / Steps 25.00\n",
      "Reward stats:\n",
      " {'max': '42665572.00',\n",
      " 'mean': '1778805.88',\n",
      " 'min': '-0.09',\n",
      " 'std': '5284479.50'}\n",
      "Information gain stats:\n",
      " {'max': '463.36', 'mean': '129.30', 'min': '6.14', 'std': '137.71'}\n",
      "Episode time 16.70\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 42 ===\n",
      "Training on [2717/8151] data points\n",
      "> Train epoch 20 [ensemble -87.44 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.25 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.51 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.58 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.63 | reward 0.00]\n",
      "Ensemble loss -110.63 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [42] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 32.00\n",
      "Reward stats:\n",
      " {'max': '66500680.00',\n",
      " 'mean': '1513171.88',\n",
      " 'min': '-0.01',\n",
      " 'std': '5777876.00'}\n",
      "Information gain stats:\n",
      " {'max': '394.27', 'mean': '79.23', 'min': '6.39', 'std': '102.61'}\n",
      "Episode time 18.85\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 43 ===\n",
      "Training on [2749/8247] data points\n",
      "> Train epoch 20 [ensemble -88.02 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.65 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.82 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.84 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.83 | reward 0.00]\n",
      "Ensemble loss -110.83 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [43] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 41.00\n",
      "Reward stats:\n",
      " {'max': '6784376.50', 'mean': '83070.84', 'min': '-0.09', 'std': '385338.34'}\n",
      "Information gain stats:\n",
      " {'max': '351.02', 'mean': '49.33', 'min': '6.31', 'std': '74.99'}\n",
      "Episode time 21.64\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 44 ===\n",
      "Training on [2790/8370] data points\n",
      "> Train epoch 20 [ensemble -87.93 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.63 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.79 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.84 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.76 | reward 0.00]\n",
      "Ensemble loss -110.76 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [44] ===\n",
      "Rewards 1.00 / Steps 23.00\n",
      "Reward stats:\n",
      " {'max': '41390180.00',\n",
      " 'mean': '2040616.50',\n",
      " 'min': '-0.02',\n",
      " 'std': '5749816.50'}\n",
      "Information gain stats:\n",
      " {'max': '453.98', 'mean': '136.23', 'min': '8.81', 'std': '132.59'}\n",
      "Episode time 16.42\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 45 ===\n",
      "Training on [2813/8439] data points\n",
      "> Train epoch 20 [ensemble -88.05 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -100.79 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -105.94 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -108.94 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -110.93 | reward 0.00]\n",
      "Ensemble loss -110.93 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [45] ===\n",
      "> Step 25 [reward 0.00]\n",
      "> Step 50 [reward 0.00]\n",
      "> Step 75 [reward 0.00]\n",
      "> Step 100 [reward 0.00]\n",
      "> Step 125 [reward 0.00]\n",
      "> Step 150 [reward 0.00]\n",
      "Rewards 0.00 / Steps 167.00\n",
      "Reward stats:\n",
      " {'max': '0.10', 'mean': '0.01', 'min': '-0.07', 'std': '0.01'}\n",
      "Information gain stats:\n",
      " {'max': '28.64', 'mean': '16.13', 'min': '5.67', 'std': '2.58'}\n",
      "Episode time 54.22\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 46 ===\n",
      "Training on [2980/8940] data points\n",
      "> Train epoch 20 [ensemble -89.71 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -101.71 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -106.64 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -109.53 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -111.36 | reward 0.00]\n",
      "Ensemble loss -111.36 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [46] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 27.00\n",
      "Reward stats:\n",
      " {'max': '36015732.00', 'mean': '357425.88', 'min': '-0.00', 'std': '1544820.88'}\n",
      "Information gain stats:\n",
      " {'max': '351.29', 'mean': '66.29', 'min': '6.58', 'std': '83.45'}\n",
      "Episode time 18.25\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 47 ===\n",
      "Training on [3007/9021] data points\n",
      "> Train epoch 20 [ensemble -89.44 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -101.54 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -106.49 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -109.27 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -111.14 | reward 0.00]\n",
      "Ensemble loss -111.14 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [47] ===\n",
      "> Step 25 [reward 1.00]\n",
      "Rewards 1.00 / Steps 25.00\n",
      "Reward stats:\n",
      " {'max': '12900385.00', 'mean': '386297.97', 'min': '-0.11', 'std': '1256786.38'}\n",
      "Information gain stats:\n",
      " {'max': '394.31', 'mean': '95.26', 'min': '5.45', 'std': '107.32'}\n",
      "Episode time 17.49\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 48 ===\n",
      "Training on [3032/9096] data points\n",
      "> Train epoch 20 [ensemble -90.20 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -102.02 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -106.83 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -109.66 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -111.47 | reward 0.00]\n",
      "Ensemble loss -111.47 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [48] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 49.00\n",
      "Reward stats:\n",
      " {'max': '62829344.00',\n",
      " 'mean': '1150936.38',\n",
      " 'min': '-0.40',\n",
      " 'std': '5115213.00'}\n",
      "Information gain stats:\n",
      " {'max': '453.21', 'mean': '77.32', 'min': '5.54', 'std': '105.69'}\n",
      "Episode time 23.89\n",
      "Saved _metrics_\n",
      "\n",
      "=== Episode 49 ===\n",
      "Training on [3081/9243] data points\n",
      "> Train epoch 20 [ensemble -90.26 | reward 0.00]\n",
      "> Train epoch 40 [ensemble -102.18 | reward 0.00]\n",
      "> Train epoch 60 [ensemble -107.02 | reward 0.00]\n",
      "> Train epoch 80 [ensemble -109.66 | reward 0.00]\n",
      "> Train epoch 100 [ensemble -111.50 | reward 0.00]\n",
      "Ensemble loss -111.50 / Reward Loss 0.00\n",
      "\n",
      "=== Collecting data [49] ===\n",
      "> Step 25 [reward 0.00]\n",
      "Rewards 1.00 / Steps 47.00\n",
      "Reward stats:\n",
      " {'max': '65146256.00',\n",
      " 'mean': '1118727.50',\n",
      " 'min': '-0.02',\n",
      " 'std': '4989215.50'}\n",
      "Information gain stats:\n",
      " {'max': '439.18', 'mean': '63.07', 'min': '6.39', 'std': '100.78'}\n",
      "Episode time 23.52\n",
      "Saved _metrics_\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--logdir\", type=str, default=\"log\")\n",
    "parser.add_argument(\"--config_name\", type=str, default=\"mountain_car\")\n",
    "parser.add_argument(\"--strategy\", type=str, default=\"information\")\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "args = parser.parse_args(args=[])\n",
    "config = get_config(args)\n",
    "config.n_episodes=10\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
